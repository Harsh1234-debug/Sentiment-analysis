{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590c7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadrantdaywise():\n",
    "    import random\n",
    "    import requests\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings()\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import string\n",
    "    import time\n",
    "    import win32com.client\n",
    "    from datetime import date, timedelta\n",
    "    import os\n",
    "    from os import listdir\n",
    "    from os.path import join\n",
    "    import shutil\n",
    "    import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    import ssl\n",
    "\n",
    "    -\n",
    "    try:\n",
    "        now = datetime.datetime.now()\n",
    "        start_date = \"{}\".format((now-timedelta(hours=24, seconds=0, minutes=0)).strftime(\"%Y-%m-%d\"))\n",
    "#         start_date = f'2021-09-0{i}'\n",
    "        end_date = start_date\n",
    "        data =  {\n",
    "            \"type\": \"per_day\",\n",
    "            \"survey_id\":[85],\n",
    "            \"limit\": 1\n",
    "        }\n",
    "        \n",
    "        proxies = ['http://192.168.5.37:8080',\n",
    "                   'http://192.168.5.38:8080',\n",
    "                   'http://192.168.5.39:8080',\n",
    "                   'http://192.168.5.40:8080',\n",
    "                   'http://192.168.5.41:8080',\n",
    "                   'http://192.168.5.42:8080',\n",
    "                   'http://192.168.5.43:8080',\n",
    "                   'http://192.168.5.44:8080',\n",
    "                   'http://192.168.5.45:8080',\n",
    "                   'http://192.168.5.46:8080']\n",
    "\n",
    "        import random\n",
    "        a = random.randrange(len(proxies))\n",
    "        proxy = {\"http\" : f\"{proxies[a]}\"}\n",
    "        \n",
    "        res = requests.post('https://npci.surveycxm.com:3000/survey/get-merge-xlsx-file',json =data ,verify = False, proxies=proxy)\n",
    "\n",
    "        url = res.json()['data'][0]['file_url']\n",
    "        df = pd.read_excel(url)\n",
    "\n",
    "        def dotask(df):\n",
    "            def clean_text(text ): \n",
    "                delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "                delete_dict[' '] = ' ' \n",
    "                table = str.maketrans(delete_dict)\n",
    "                text1 = text.translate(table)\n",
    "                #print('cleaned:'+text1)\n",
    "                textArr= text1.split()\n",
    "                text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "                return text2.lower()\n",
    "\n",
    "\n",
    "            df['Remarks'] =df['Remarks'].astype(str)\n",
    "            df['Cleaned_Remarks'] = ''\n",
    "            df['Cleaned_Remarks'] =df['Remarks'].apply(clean_text)\n",
    "            df['Categories'] = ''\n",
    "            df['Sub Categories'] = ''\n",
    "\n",
    "\n",
    "            Happy_Customers     = ['very good ','simple transfer ','easy ','good ','efficient way ','fast app ','more accessible ','better processing ',\n",
    "                                   'very effective ','fast ','user friendly ','best upi ','quite efficient ','so simple ','very safe ','indian ',\n",
    "                                   'very quickly ','best ','nice ','very satisfied ','happy ','love ','just wonderful ','super ','awesome ','best performance ',\n",
    "                                   'excellent ','nice app ','for pay bill easily ','nice service ','best customer experience ','satisfied ','better app ',\n",
    "                                   'very nice ','excellent ','fabulous ','awsome ','awesome ','cool ', 'very good',' simple transfer',' easy',' good',' efficient way',' fast app',' more accessible',' better processing',\n",
    "                                   ' very effective',' fast',' user friendly',' best upi',' quite efficient',' so simple',' very safe',' indian',\n",
    "                                   ' very quickly',' best',' nice',' very satisfied',' happy',' love',' just wonderful',' super',' awesome',' best performance',\n",
    "                                   ' excellent',' nice app',' for pay bill easily',' nice service',' best customer experience',' satisfied',' better app',\n",
    "                                   ' very nice',' excellent',' fabulous',' awsome',' awesome',' cool']\n",
    "            Transaction                  = ['transaction ','transition ','transection ',' transaction',' transition',' transection']\n",
    "            Recharge                     = ['recharge ','recharging ',' recharge',' recharging']\n",
    "            Comparing                    = ['works faster than ','compare ','way better than ','better than ','googlepay ','google pay ','paytm ','phonepay ', 'phone pay ','mi ','other upi ',\n",
    "                                           'mobikwik ','gpay ','phonepes ','paypal ',' works faster than',' compare',' way better than',' better than',' googlepay',\n",
    "                                            ' google pay',' paytm',' phonepay',' phone pay',' mi',' other upi',' mobikwik',' gpay',' phonepes',' paypal']\n",
    "            Interface                    = ['interface ','inter face ',' interface',' inter face']\n",
    "            Server                       = ['very slow ','speed ','server ','network issue ','low speed','servsr ',' very slow',' speed',' server',' network issue',' low speed',' servsr']\n",
    "            Complaints                   = ['complaint ','fraud ','upi pin ','complaints ','unblock ','thing went wrong ','disabled ','complaint ',\n",
    "                                            'fraud ','upi pin ','complaints ','unblock ','thing went wrong ','disabled ']\n",
    "            Isses_Not_Solved             =  ['issue ','issues ','not resolved ',' issue',' issues',' not resolved']\n",
    "            Offers_And_Reward            =  ['reward ','discount ','offer ','rewards ','cashback ','scratch card ','cashbach ','vouchers ','cashbacks ','cash back ','coupons ',\n",
    "                                              'redeem card ',' reward',' discount',' offer',' rewards',' cashback',' scratch card',' cashbach',' vouchers',' cashbacks',' cash back',' coupons',\n",
    "                                            ' redeem card']\n",
    "            Customer_Service             =  ['customer care ','customer service ','customer support ','toll free ','customers care ', ' customer care',' customer service',' customer support',\n",
    "                                           ' toll free',' customers care']\n",
    "            Suggestion_For_Improvement   = ['add ','improve ','improvement ','improvements ','start new ','more options ','more option ',\n",
    "                                           'Make better ','feedback given by ','more update ','improvment ','more facilities ','booking features ',' add',' improve',' improvement',\n",
    "                                            ' improvements',' start new',' more options',' more option', ' Make better',' feedback given by',' more update',' improvment',\n",
    "                                           ' more facilities',' booking features']\n",
    "            QR_Code                      = [' qr','qr ']\n",
    "            Security                     = ['security ','security issues ',' security',' security issues']\n",
    "            Facing_Problem_In_App        = [' can not see',' cannt see',' cant see', 'can not see ','cannt see ','cant see ']\n",
    "            Related_to_Bugs              = ['bugs ','bug ','fix it ',' bugs',' bug',' fix it']\n",
    "            Not_Happy_With_App           = ['hopeless ','pathetic ','hate ','bad app ','not good ','app not working ',' hopeless',' pathetic',' hate',' bad app',' not good',' app not working']\n",
    "            Not_Able_To_Login            =['login ','log in ','sign in ','signin ','sim ','late ','verfication ',' login',' log in',' sign in',' signin',' sim',' late',' verfication']\n",
    "            IPO                          = ['ipo ',' ipo']\n",
    "\n",
    "\n",
    "            for i in range(0, len(df.Cleaned_Remarks)):\n",
    "                if any(word in df.Cleaned_Remarks[i] for word in Happy_Customers):\n",
    "                    df['Categories'][i] =  'Happy Customers'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Transaction):\n",
    "                    df['Categories'][i] = 'Transaction'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Recharge):\n",
    "                    df['Categories'][i] = 'Recharge'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Comparing):\n",
    "                    df['Categories'][i] = 'Comparing'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Interface):\n",
    "                    df['Categories'][i] = 'Interface'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Server):\n",
    "                    df['Categories'][i] = 'Server'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Complaints):\n",
    "                    df['Categories'][i] = 'Complaints'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Isses_Not_Solved):\n",
    "                    df['Categories'][i] = 'Isses_Not_Solved'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Offers_And_Reward):\n",
    "                    df['Categories'][i] = 'Offers_And_Reward'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Customer_Service):\n",
    "                    df['Categories'][i] = 'Customer_Service'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Suggestion_For_Improvement):\n",
    "                    df['Categories'][i] = 'Suggestion For Improvement'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in QR_Code):\n",
    "                    df['Categories'][i] = 'QR Code'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Security):\n",
    "                    df['Categories'][i] = 'Security'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Facing_Problem_In_App):\n",
    "                    df['Categories'][i] = 'Facing Problem In App'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Related_to_Bugs):\n",
    "                    df['Categories'][i] = 'Related to Bugs'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Not_Happy_With_App):\n",
    "                    df['Categories'][i] = 'Not Happy With App'\n",
    "\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in Not_Able_To_Login):\n",
    "                    df['Categories'][i] = 'Not Able To Login'\n",
    "\n",
    "                elif any(word in df.Cleaned_Remarks[i] for word in IPO):\n",
    "                    df['Categories'][i] = 'IPO'\n",
    "\n",
    "\n",
    "            Transaction = df[df.Categories=='Transaction']\n",
    "            Transaction =Transaction.reset_index()\n",
    "            Transaction = Transaction.drop(['index'],axis =1)\n",
    "\n",
    "            Not_able_to_do_transcation = ['mobile number has been changed','transfer','payement not','payment not','processed',\n",
    "                                          'can not debited','cant debited','can not debited']\n",
    "            wrong_transaction = ['wrong transaction','mistakly wrong transaction']\n",
    "            money_dedected = ['detected','deduct','deducted']\n",
    "            transaction_limit = ['limit','trasaction limit']\n",
    "            payment_failure = ['failure','fails','payment failure','payment fails']\n",
    "            not_refund = ['not refund','refund','refunding','refunds']\n",
    "            error = ['error','errors']\n",
    "            other_issue = ['issue','other issue','issues']\n",
    "            pending = ['pending']\n",
    "            rejection_in_payment = ['rejection']\n",
    "\n",
    "            for i in range(0, len(Transaction.Cleaned_Remarks)):\n",
    "                if any(word in Transaction.Cleaned_Remarks[i] for word in Not_able_to_do_transcation):\n",
    "                    Transaction['Sub Categories'][i] =  'Not_able_to_do_transcation'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in wrong_transaction):\n",
    "                    Transaction['Sub Categories'][i] = 'wrong transaction'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in money_dedected):\n",
    "                    Transaction['Sub Categories'][i] = 'money dedected'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in transaction_limit):\n",
    "                    Transaction['Sub Categories'][i] = 'transaction limit'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in payment_failure):\n",
    "                    Transaction['Sub Categories'][i] = 'payment failure'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in not_refund):\n",
    "                    Transaction['Sub Categories'][i] = 'not refund'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in error):\n",
    "                    Transaction['Sub Categories'][i] = 'error'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in other_issue):\n",
    "                    Transaction['Sub Categories'][i] = 'other issue'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in pending):\n",
    "                    Transaction['Sub Categories'][i] = 'pending'\n",
    "\n",
    "                elif any(word in Transaction.Cleaned_Remarks[i] for word in rejection_in_payment):\n",
    "                    Transaction['Sub Categories'][i] = 'rejection in payment'\n",
    "\n",
    "\n",
    "\n",
    "            Happy_Customer = df[df.Categories=='Happy Customers']\n",
    "            Happy_Customer =Happy_Customer.reset_index()\n",
    "            Happy_Customer = Happy_Customer.drop(['index'],axis =1)\n",
    "\n",
    "            cashback = ['cashback','offer','reward']\n",
    "            interface = ['interface']\n",
    "            security = ['secuity','secure']\n",
    "            trust_bcz_gov_support = ['indian','govt','government']\n",
    "            userfriendlyapp = ['easy','user friendly','app','application']\n",
    "\n",
    "\n",
    "            for i in range(0, len(Happy_Customer.Cleaned_Remarks)):\n",
    "                if any(word in Happy_Customer.Cleaned_Remarks[i] for word in cashback):\n",
    "                    Happy_Customer['Sub Categories'][i] =  'Cashback/offer Related'\n",
    "\n",
    "                elif any(word in Happy_Customer.Cleaned_Remarks[i] for word in interface):\n",
    "                    Happy_Customer['Sub Categories'][i] = 'Interface'\n",
    "\n",
    "                elif any(word in Happy_Customer.Cleaned_Remarks[i] for word in security):\n",
    "                    Happy_Customer['Sub Categories'][i] = 'Security'\n",
    "\n",
    "                elif any(word in Happy_Customer.Cleaned_Remarks[i] for word in trust_bcz_gov_support):\n",
    "                    Happy_Customer['Sub Categories'][i] = 'Trust Because of Govt Support'\n",
    "\n",
    "                elif any(word in Happy_Customer.Cleaned_Remarks[i] for word in userfriendlyapp):\n",
    "                    Happy_Customer['Sub Categories'][i] = 'User Friendly App'\n",
    "            Happy_Customer['Sub Categories'] = Happy_Customer['Sub Categories'].replace(np.nan, 'Generic Comment')\n",
    "\n",
    "            df_ = pd.concat([Transaction, df,Happy_Customer]).drop_duplicates().reset_index(drop=True)\n",
    "            df_ = df_.sample(frac=1).reset_index(drop=True)\n",
    "            df_ = df_[['id','product','Remarks','vendor_name','vendor_id','survey_response_id','state','city','rating_response',\n",
    "               'rating_response_datetime','rating','survey_response','survey_response_datetime','Categories',\n",
    "               'Sub Categories']]\n",
    "            df_['Categories'] = df_['Categories'].replace('', np.nan, regex=True)\n",
    "            df_['Categories'] = df_['Categories'].replace(np.nan, 'Unproductive Comment')\n",
    "            df_.reset_index(drop=True, inplace=True)\n",
    "            col = ['Unproductive Comment']\n",
    "            df_ = df_[~df_['Categories'].isin(col)]\n",
    "            df =df_\n",
    "\n",
    "            df['NPS CAT']= \"\"\n",
    "            detractor = df[df['rating'] < 7]\n",
    "            detractor['NPS CAT'] = \"Detractor\"\n",
    "            promoter =df[df['rating'] > 8]\n",
    "            promoter['NPS CAT'] = \"Promoter\"\n",
    "            passive = df[(df['rating'] < 9) & (df['rating'] > 6)]\n",
    "            passive['NPS CAT'] = \"passive\"\n",
    "\n",
    "            frames = [detractor,promoter,passive]\n",
    "            final = pd.concat(frames)\n",
    "\n",
    "            options = ['Unproductive Comment']\n",
    "            final_imp_cat = final.loc[~final['Categories'].isin(options)]\n",
    "\n",
    "            detractor = final_imp_cat[final_imp_cat['rating'] < 7]\n",
    "            detractor['NPS CAT'] = \"Detractor\"\n",
    "            promoter =final_imp_cat[final_imp_cat['rating'] > 8]\n",
    "            promoter['NPS CAT'] = \"Promoter\"\n",
    "            passive = final_imp_cat[(final_imp_cat['rating'] < 9) & (final_imp_cat['rating'] > 6)]\n",
    "            passive['NPS CAT'] = \"Passive\"\n",
    "\n",
    "\n",
    "            frames = [detractor,promoter,passive]\n",
    "            final_ = pd.concat(frames)\n",
    "\n",
    "            Promoter_count  = len(final_[final_['NPS CAT']=='Promoter'])\n",
    "            passive_count   = len(final_[final_['NPS CAT']=='Passive'])\n",
    "            Detractor_count = len(final_[final_['NPS CAT']=='Detractor'])\n",
    "\n",
    "            final_['Total Nps Score'] = ''\n",
    "            final_['Total Nps Score'] = (Promoter_count-Detractor_count)/(Promoter_count+passive_count+Detractor_count)*100\n",
    "\n",
    "            col = ['Unnamed: 0', 'id', 'product', 'Remarks', 'vendor_name', 'vendor_id',\n",
    "                  'survey_response_id', 'state', 'city', 'rating_response',\n",
    "                  'rating_response_datetime', 'rating', 'survey_response',\n",
    "                  'survey_response_datetime', 'Categories', 'Sub Categories',\n",
    "                  'NPS CAT', 'Total Nps Score', 'Total Nps Score Cat Wise']\n",
    "            df = pd.DataFrame(columns=col)\n",
    "\n",
    "            for i in final_['Categories'].unique():\n",
    "                j=i\n",
    "                j = final_[final_['Categories'] == i]\n",
    "                detractor = j[j['rating'] < 7]\n",
    "                detractor['NPS CAT'] = \"Detractor\"\n",
    "                promoter =j[j['rating'] > 8]\n",
    "                promoter['NPS CAT'] = \"Promoter\"\n",
    "                passive = j[(j['rating'] < 9) & (j['rating'] > 6)]\n",
    "                passive['NPS CAT'] = \"Passive\"\n",
    "\n",
    "                frames = [detractor,promoter,passive]\n",
    "                a = pd.concat(frames)\n",
    "                Promoter_count  = len(a[a['NPS CAT']=='Promoter'])\n",
    "                passive_count   = len(a[a['NPS CAT']=='Passive'])\n",
    "                Detractor_count = len(a[a['NPS CAT']=='Detractor'])\n",
    "\n",
    "\n",
    "                a['Total Nps Score Cat Wise'] = ''\n",
    "                a['Total Nps Score Cat Wise'] = (Promoter_count-Detractor_count)/(Promoter_count+passive_count+Detractor_count)*100\n",
    "                frames = [df,a]\n",
    "                df = pd.concat(frames)\n",
    "\n",
    "            final_df = df[['Categories','rating']]\n",
    "            final_df['rating']= final_df['rating'].astype(int)\n",
    "            X = final_df.select_dtypes(include=[object])\n",
    "            Y = final_df['rating']\n",
    "            X = X.copy()\n",
    "            X = pd.get_dummies(X['Categories'])\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "            model = XGBClassifier()\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            def importance_vs_feature_plot(columns, importances):\n",
    "                df = (pd.DataFrame({'feature' : columns,\n",
    "                                   'feature importances' : importances})\n",
    "                    .sort_values('feature importances', ascending=True)\n",
    "                    .reset_index(drop=True))\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.barh(df['feature'], df['feature importances'])\n",
    "\n",
    "            importance_vs_feature_plot( X_train.columns, model.feature_importances_ )\n",
    "\n",
    "            output_df = pd.DataFrame({\"Categories\" : X_train.columns,\n",
    "                                     \"Impact of Categories on NPS\" : model.feature_importances_}).sort_values(by=\"Impact of Categories on NPS\", ascending=False)\n",
    "            df1 = pd.merge(df, output_df, on = 'Categories', how = 'left')\n",
    "\n",
    "            df2 = df1[df1[\"Categories\"]=='Transaction']\n",
    "            df2 = df2.replace(\"\",np.nan)\n",
    "            df2.dropna(subset = [\"Sub Categories\"], inplace=True)\n",
    "\n",
    "            X = pd.get_dummies(df2[\"Sub Categories\"]).reset_index(drop=True)\n",
    "            Y = df2[\"rating\"]\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "            model = XGBClassifier()\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "            def importance_vs_feature_plot(columns, importances):\n",
    "                df = (pd.DataFrame({'feature' : columns,\n",
    "                                   'feature importances' : importances})\n",
    "                    .sort_values('feature importances', ascending=True)\n",
    "                    .reset_index(drop=True))\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.barh(df['feature'], df['feature importances'])\n",
    "\n",
    "            importance_vs_feature_plot( X_train.columns, model.feature_importances_ )\n",
    "\n",
    "            output_df_ = pd.DataFrame({\"Sub Categories\" : X_train.columns,\n",
    "                                     \"Impact of Sub Categories on NPS\" : model.feature_importances_}).sort_values(by=\"Impact of Sub Categories on NPS\", ascending=False)\n",
    "\n",
    "            df3 = pd.merge(df1, output_df_, on = 'Sub Categories', how = 'left')\n",
    "            df3 = df3.drop(['Unnamed: 0'],axis= 1)\n",
    "\n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            df3.to_csv(fr'E:\\DIvyam\\BHIM\\quad_day_wise\\result{now.strftime(\"%d_%m_%Y %H_%M_%S\")}.csv')\n",
    "            files = [join(r\"E:\\DIvyam\\BHIM\\quad_day_wise\", f) for f in listdir(r\"E:\\DIvyam\\BHIM\\quad_day_wise\")]\n",
    "            path = files[0]\n",
    "            url = \"https://npci.surveycxm.com:3000/survey/update-quadrant-data\"\n",
    "    #         res = requests.post(url,verify = False, files = {'quadrant_file' :  open(path, 'rb'),'start_date' : '2021-08-20','end_date':'2021-08-20'})\n",
    "            res = requests.post(url,verify = False, files = {'quadrant_file' :  open(path, 'rb')}, data = {'DB':'photcat','OUT':'csv','SHORT':\n",
    "                                                                                                          'short','start_date' : start_date,'end_date':end_date})\n",
    "            print(res.status_code)\n",
    "            try:\n",
    "                for f in files:\n",
    "                    shutil.move(f,'E:\\\\DIvyam\\\\BHIM\\\\data')\n",
    "            except:\n",
    "                pass\n",
    "        dotask(df)\n",
    "        print(now)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c895ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 1 day at 03:15:00 do quadrantdaywise() (last run: [never], next run: 2021-10-02 03:15:00)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "schedule.every().day.at(\"03:15\").do(quadrantdaywise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb22734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:15:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:15:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "200\n",
      "2021-10-02 03:15:10.521801\n",
      "[03:15:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:15:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "200\n",
      "2021-10-03 03:15:00.870534\n",
      "[03:15:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:15:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "200\n",
      "2021-10-04 03:15:04.573227\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b7a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'npci.surveycxm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "start_date = \"{}\".format((now-timedelta(hours=24, seconds=0, minutes=0)).strftime(\"%Y-%m-%d\"))\n",
    "#         start_date = f'2021-09-0{i}'\n",
    "end_date = start_date\n",
    "data =  {\n",
    "    \"type\": \"per_day\",\n",
    "    \"survey_id\":[85],\n",
    "    \"limit\": 1\n",
    "}\n",
    "print(start_date)\n",
    "res = requests.post('https://npci.surveycxm.com:3000/survey/get-merge-xlsx-file',json =data ,verify = False)\n",
    "\n",
    "url = res.json()['data'][0]['file_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a140127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae3ab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'code': 200,\n",
       " 'message': 'OK',\n",
       " 'data': [{'id': 860,\n",
       "   'type': 'per_day',\n",
       "   'start_date': '2021-09-30 00:00:00',\n",
       "   'end_date': '2021-09-30 23:59:59',\n",
       "   'filename': '2021-09-30000000_2021-09-30235959_1633032000399.xlsx',\n",
       "   'file_url': 'https://npci.surveycxm.com:3000/free_text_dump_data/2021-09-30000000_2021-09-30235959_1633032000399.xlsx',\n",
       "   'survey_id': 85,\n",
       "   'created_at': '2021-09-30T20:00:00.000Z'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141a12b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
